{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "cell_execution_strategy": "setup",
      "authorship_tag": "ABX9TyNfTClXph2V1nMnqmmXtq2K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijay313v/c/blob/main/Copy_of_Cardiovascular_health_risk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project title: Cardiovascular risk prediction**"
      ],
      "metadata": {
        "id": "E6R5Q8SLEx6T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Description:**\n",
        "Cardiovascular diseases (CVDs) are the leading cause of death globally, taking an estimated 18.6 million lives each year, which accounts for 33% of all the global deaths. CVDs are a group of disorders of the heart and blood vessels and include coronary heart disease, cerebrovascular disease, rheumatic heart disease and other conditions. More than four out of five CVD deaths are due to heart attacks and strokes, and one third of these deaths occur prematurely in people under 70 years of age.\n",
        "\n",
        "It is important to detect cardiovascular disease as early as possible so that management with counselling and medicines can begin.\n",
        "\n",
        "Our main aim here is to predict whether the patient has a 10-year risk of future coronary heart disease (CHD)."
      ],
      "metadata": {
        "id": "vwlclbIaE7P1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data description:**\n",
        "The problem and aim stated above can be solved with the help of machine learning models and the data that we have. The dataset is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. The dataset provides the patients’ information. It includes over 4,000 records and 15 attributes."
      ],
      "metadata": {
        "id": "nJcv_tqmFV5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Defining the columns:**"
      ],
      "metadata": {
        "id": "ERJeSScOFxcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Demographic:**\n",
        "• Sex: male or female(\"M\" or \"F\")\n",
        "\n",
        "• Age: Age of the patient;(Continuous - Although the recorded ages have been truncated to whole numbers, the concept of age is continuous)"
      ],
      "metadata": {
        "id": "MsYp6D0UF5KG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Behavioral:**\n",
        "• is_smoking: whether or not the patient is a current smoker (\"YES\" or \"NO\")\n",
        "\n",
        "• Cigs Per Day: the number of cigarettes that the person smoked on average in one day.(can be considered continuous as one can have any number of cigarettes, even half a cigarette.)"
      ],
      "metadata": {
        "id": "P6XvA8vgGFqX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Medical( history):**\n",
        "• BP Meds: whether or not the patient was on blood pressure medication (Nominal)\n",
        "\n",
        "• Prevalent Stroke: whether or not the patient had previously had a stroke (Nominal)\n",
        "\n",
        "• Prevalent Hyp: whether or not the patient was hypertensive (Nominal)\n",
        "\n",
        "• Diabetes: whether or not the patient had diabetes (Nominal)"
      ],
      "metadata": {
        "id": "qKy5eVO_GR2I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Medical(current):**\n",
        "• Tot Chol: total cholesterol level (Continuous)\n",
        "\n",
        "• Sys BP: systolic blood pressure (Continuous)\n",
        "\n",
        "• Dia BP: diastolic blood pressure (Continuous)\n",
        "\n",
        "• BMI: Body Mass Index (Continuous)\n",
        "\n",
        "• Heart Rate: heart rate (Continuous - In medical research, variables such as heart rate though in fact discrete, yet are considered continuous because of large number of possible values.)\n",
        "\n",
        "• Glucose: glucose level (Continuous)"
      ],
      "metadata": {
        "id": "BNcaOdj1Gbpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Predict variable (desired target):**\n",
        "• 10-year risk of coronary heart disease CHD(binary: “1”, means “Yes”, “0” means “No”)"
      ],
      "metadata": {
        "id": "UO1mR83zGl1B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jSga2tREjrQ"
      },
      "outputs": [],
      "source": [
        "#Importing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report,roc_auc_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mounting the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CVOCNbiPGy85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading dataset\n",
        "cv_df =pd.read_csv(r\"/content/data_cardiovascular_risk (1).csv\")"
      ],
      "metadata": {
        "id": "fHNh_v0QGy5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shape of dataset\n",
        "cv_df.shape"
      ],
      "metadata": {
        "id": "Z48E-qqXHPhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3390 observation and 17 columns**"
      ],
      "metadata": {
        "id": "H0rQP4HzHTTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#summary of datset\n",
        "cv_df.info()"
      ],
      "metadata": {
        "id": "NZ05qPjbHPdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#statistical description of dataframe\n",
        "cv_df.describe()"
      ],
      "metadata": {
        "id": "kJCLZ7KyHcCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the distribution of the target variable\n",
        "cv_df['TenYearCHD'].value_counts()"
      ],
      "metadata": {
        "id": "f4Shvnt3Hb_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EDA:**"
      ],
      "metadata": {
        "id": "_EMqPx42H4Gn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Which Age people has high chances of positive CHD.**"
      ],
      "metadata": {
        "id": "jeb3OavwH4Cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Analysis on Age column\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Create a custom palette\n",
        "my_palette = {0: 'blue', 1: 'orange'}\n",
        "\n",
        "# Create the countplot\n",
        "ax = sns.countplot(x=cv_df['age'], hue=cv_df['TenYearCHD'], palette=my_palette)\n",
        "\n",
        "plt.title(' Which Age more prone to CHD')\n",
        "plt.legend(['No Risk', 'At Risk'])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vkiOQVO4Hb8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Age between 47 to 65 has high chances of positive CHD.**"
      ],
      "metadata": {
        "id": "BQNG-cLIIMkS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Education:**"
      ],
      "metadata": {
        "id": "LKF9s6m7IV4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Analysis on Education column\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Create a custom palette\n",
        "my_palette = {0: 'blue', 1: 'orange'}\n",
        "\n",
        "# Create the countplot\n",
        "ax = sns.countplot(x=cv_df['education'], hue=cv_df['TenYearCHD'], palette=my_palette)\n",
        "\n",
        "plt.title(' Which Age more prone to CHD')\n",
        "plt.legend(['No Risk', 'At Risk'])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NKIR7dyLHb56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As we can see that, most of the \"At Risk\" cases are in the 1st level of education and least in the 4th level. This is a pretty misleading result because the number of \"At Risk\" cases here seems to be affected by the total number of people in that category. Therefore we can infer that this is not a good comparitive point.**"
      ],
      "metadata": {
        "id": "-RFqKJ9lIkV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Which sex is most likely to suffer from positive CHD.**"
      ],
      "metadata": {
        "id": "CLFpXniNKtOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysis of sex column\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Create a custom palette\n",
        "my_palette = {0: 'blue', 1: 'orange'}\n",
        "\n",
        "# Create the countplot\n",
        "ax = sns.countplot(x=cv_df['sex'], hue=cv_df['TenYearCHD'], palette=my_palette)\n",
        "\n",
        "plt.title('Gender more prone to CHD')\n",
        "plt.legend(['No Risk', 'At Risk'])\n",
        "\n",
        "# Calculate and add percentage text to the plot\n",
        "total = len(cv_df)\n",
        "for p in ax.patches:\n",
        "    height = p.get_height()\n",
        "    ax.text(p.get_x() + p.get_width() / 2, height + 5, f'{height/total*100:.2f}%', ha='center')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a-tRiLsXK5SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Being a Male has high chances of CHD compare to Female.**"
      ],
      "metadata": {
        "id": "QL9a5CHzK-sb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **is_smoking**"
      ],
      "metadata": {
        "id": "6LP2m7mbLDbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysis on is_smoking\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Create a custom palette\n",
        "my_palette = {0: 'blue', 1: 'orange'}\n",
        "\n",
        "# Create the countplot\n",
        "ax = sns.countplot(x=cv_df['is_smoking'], hue=cv_df['TenYearCHD'], palette=my_palette)\n",
        "\n",
        "plt.title(' ')\n",
        "plt.legend(['No Risk', 'At Risk'])\n",
        "\n",
        "# Calculate and add percentage text to the plot\n",
        "total = len(cv_df)\n",
        "for p in ax.patches:\n",
        "    height = p.get_height()\n",
        "    ax.text(p.get_x() + p.get_width() / 2, height + 5, f'{height/total*100:.2f}%', ha='center')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vIpYqR73LI4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Smoking increases the chances of positive CHD.**"
      ],
      "metadata": {
        "id": "EjKT7TJzLODw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Does people taking BPMeds effect on CHD**"
      ],
      "metadata": {
        "id": "jilBxQ0WLTK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysis on BPMeds\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Create a custom palette\n",
        "my_palette = {0: 'blue', 1: 'orange'}\n",
        "\n",
        "# Create the countplot\n",
        "ax = sns.countplot(x=cv_df['BPMeds'], hue=cv_df['TenYearCHD'], palette=my_palette)\n",
        "\n",
        "plt.title(' ')\n",
        "plt.legend(['No Risk', 'At Risk'])\n",
        "\n",
        "# Calculate and add percentage text to the plot\n",
        "total = len(cv_df)\n",
        "for p in ax.patches:\n",
        "    height = p.get_height()\n",
        "    ax.text(p.get_x() + p.get_width() / 2, height + 5, f'{height/total*100:.2f}%', ha='center')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dy9p5vN1LaEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**erson taking BP medicines has high chances of CHD.**"
      ],
      "metadata": {
        "id": "ZuPb0A2whIk-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **How does prevalent Stroke effect in positive CHD factor.**"
      ],
      "metadata": {
        "id": "Q88xYvhqhOKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Analysis on prevalent stroke column\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Create a custom palette\n",
        "my_palette = {0: 'blue', 1: 'orange'}\n",
        "\n",
        "# Create the countplot\n",
        "ax = sns.countplot(x=cv_df['prevalentStroke'], hue=cv_df['TenYearCHD'], palette=my_palette)\n",
        "\n",
        "plt.title(' IS prevalent Stroke effect in CHD')\n",
        "plt.legend(['No Risk', 'At Risk'])\n",
        "\n",
        "# Calculate and add percentage text to the plot\n",
        "total = len(cv_df)\n",
        "for p in ax.patches:\n",
        "    height = p.get_height()\n",
        "    ax.text(p.get_x() + p.get_width() / 2, height + 5, f'{height/total*100:.2f}%', ha='center')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "24hMvh3phTMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prevalent stroke increases chances of CHD in future.**"
      ],
      "metadata": {
        "id": "8f-Nvw8FhX3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Does Prevalent Hypertension has effect on positive CHD in future**"
      ],
      "metadata": {
        "id": "ZuTNi72qhdf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Analysis on  prevalent Hpypertension column\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Create a custom palette\n",
        "my_palette = {0: 'blue', 1: 'orange'}\n",
        "\n",
        "# Create the countplot\n",
        "ax = sns.countplot(x=cv_df['prevalentHyp'], hue=cv_df['TenYearCHD'], palette=my_palette)\n",
        "\n",
        "plt.title(' Effect of prevalent Hpypertension on CHD')\n",
        "plt.legend(['No Risk', 'At Risk'])\n",
        "\n",
        "# Calculate and add percentage text to the plot\n",
        "total = len(cv_df)\n",
        "for p in ax.patches:\n",
        "    height = p.get_height()\n",
        "    ax.text(p.get_x() + p.get_width() / 2, height + 5, f'{height/total*100:.2f}%', ha='center')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fRaqGmw-hgHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**prevalent Hypertension increases chances of CHD in patients**"
      ],
      "metadata": {
        "id": "qfgpCMUfho4k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Does Diabetes affect the chances of having a positive CHD risk factor**"
      ],
      "metadata": {
        "id": "MyK55JJEhtkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Analysis of Diabetes column\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Create a custom palette\n",
        "my_palette = {0: 'blue', 1: 'orange'}\n",
        "\n",
        "# Create the countplot\n",
        "ax = sns.countplot(x=cv_df['diabetes'], hue=cv_df['TenYearCHD'], palette=my_palette)\n",
        "\n",
        "plt.title(' Effect of Diabetes on CHD')\n",
        "plt.legend(['No Risk', 'At Risk'])\n",
        "\n",
        "# Calculate and add percentage text to the plot\n",
        "total = len(cv_df)\n",
        "for p in ax.patches:\n",
        "    height = p.get_height()\n",
        "    ax.text(p.get_x() + p.get_width() / 2, height + 5, f'{height/total*100:.2f}%', ha='center')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PvoVbzvthy1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Diabetes increases chances of positive Coronary Heart Disease**"
      ],
      "metadata": {
        "id": "WE7M0A_lh4ub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Others**"
      ],
      "metadata": {
        "id": "0i3rBIGriCoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(data=cv_df[['totChol','sysBP','diaBP','BMI','heartRate','glucose']])\n",
        "plt.title('Box Plot  of Continuous Variables')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IBGdRtzOiJYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**from above using the boxplot we get the idea of range of different columns and outliers**"
      ],
      "metadata": {
        "id": "0AlTCOITiWEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sns.displot(data=cv_df, x='totChol', hue='TenYearCHD', kind='kde', fill=True, height=5, aspect=2)\n",
        "plt.axvline(200, 0,1,color='yellow')\n",
        "plt.axvline(240, 0,1,color='red')\n",
        "plt.xlabel('total Cholestrol, (mg/dL)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UW50ta6vitye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U6DMvtcStUlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Total cholesterol includes low-density lipoprotein (bad) cholesterol and high-density lipoprotein (good) cholesterol.**\n",
        "\n",
        "**Less than 200 mg/dL is desirable level, 200 - 239 mg/dL is borderline high level and 240 mg/dL and above comes in the category of high level.**\n",
        "\n",
        "**In our dataset most of the people are either in the borderline area(between yellow and red vertical line) or in the high level(beyond red line).**\n",
        "\n",
        "**People who are at risk of CHD have total cholesterol ranging in between less than 100 mg/dL to 700 mg/dL almost.**"
      ],
      "metadata": {
        "id": "xgAfcOJHi9fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sns.displot(data=cv_df, x='glucose', hue='TenYearCHD', kind='kde', fill=True, height=5, aspect=2)\n",
        "plt.axvline(70, color='black')\n",
        "plt.axvline(140, color='black')\n",
        "plt.xlabel('glucose (mg/dL)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CNNJigbQtWTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The normal glucose ranges from 70 mg/dL to 140 mg/dL given at what time of the day it was done whether it was before meal, after meal, during fasting or before bedtime.**\n",
        "\n",
        "**It is clear from the distribution that there has been cases with glucose level as low as 20 mg/dL to 25 mg/dL and as high as 400 mg/dL to 440 mg/dL.**\n",
        "\n",
        "**Also we can see that the glucose level is touching the high and the low value for both the cases whether the risk of coronary heart disease is present or not.**"
      ],
      "metadata": {
        "id": "RUfvY8w1jSoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the displot with the modified hue\n",
        "sns.displot(data=cv_df, x='heartRate', hue='TenYearCHD', kind='kde', fill=True, height=5, aspect=2)\n",
        "plt.axvline(60, 0,1,color='black')\n",
        "plt.axvline(100, 0,1,color='black')\n",
        "plt.xlabel('heart rate (bpm)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_M8x1hF5o8HE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resting healthy heart rate for a normal human body is between 60 bpm to 100 bpm but in our dataset it ranges between 38 bpm to 155 bpm.**\n",
        "\n",
        "**In patients with known coronary heart disease, elevated heart rate reduces diastolic filling time and increases cardiac workload, resulting in supply demand mismatch with consequent ischemia(condition in which the blood flow (and thus oxygen) is restricted or reduced in a part of the body) and angina(chest pain caused by reduced blood flow to the heart).**\n",
        "\n",
        "**Surprisingly in our dataset no conclusion can be made to distinguish between the people who are at risk of CHD or not at risk as for both categories of people the heart rate varies similarly.**"
      ],
      "metadata": {
        "id": "X9OY2QzJkMiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sns.set_style(\"darkgrid\")\n",
        "sns.relplot(data=cv_df, x=\"BMI\", y=\"age\",hue=\"TenYearCHD\",col='TenYearCHD',style='TenYearCHD',kind=\"scatter\",palette='RdBu')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Lq0lKKlTkZPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BMI in our dataset ranges in between 15 to almost 60.**\n",
        "\n",
        "**People with BMI in the range 18.5 to 24.9 are considered healthy, 25.0 to 29.9 as overweight and after 30.0 are classified as obese.**\n",
        "\n",
        "**People with risk of coronary heart disease are spread quite evenly.**\n",
        "\n",
        "**So there must be other factors other than BMI that are contributing to the potential risk of coronary heart disease.We have cases where people are in the category of obese but still not at risk of CHD and a lot of people in the category of healthy but still at the risk of CHD.**"
      ],
      "metadata": {
        "id": "QPlSk1cfuwZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.displot(data=cv_df, x='sysBP', hue='TenYearCHD', kind='kde', fill=True, height=5, aspect=2)\n",
        "plt.axvline(120, 0,1,color='black')\n",
        "plt.xlabel('systolic BP (mmHg)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cP0NT7-JvAe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.displot(data=cv_df, x='diaBP', hue='TenYearCHD', kind='kde', fill=True, height=5, aspect=2)\n",
        "plt.axvline(80, 0,1,color='black')\n",
        "plt.xlabel('diabolic BP (mmHg)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZzgG_G_wvDYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**When it comes to systolic and diastolic blood pressure measurements.**\n",
        "\n",
        "**SysBP less than 120 is considered normal and DiaBP less than 80 is considered normal.**\n",
        "\n",
        "**In Systolic BP we have cases where the readings are almost 380 mmHg which is very high number. It is also visible that people that are more at risk of CHD have readings reaching upto 380 mmHg.**\n",
        "\n",
        "**Although there is no evidence to assume whether BP readings(systolic and diastolic)are contributing to the risk of CHD or not.**"
      ],
      "metadata": {
        "id": "TOOv4jj4vJ_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Cleaning :**"
      ],
      "metadata": {
        "id": "x4FvsT3_wdsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Checking for null values in our dataset\n",
        "cv_df.isnull().sum()"
      ],
      "metadata": {
        "id": "M6IJjwHZywtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'll be using a mixed approach of imputing null values with some meaningfull value and deleting the observations with null values.\n",
        "\n",
        "Since the glucose column has a lot of null values, I'll impute them with the mean glucose value. After this, the number of null values present will be of a very small order when compared to the size of the dataset, therefore I'll just delete them."
      ],
      "metadata": {
        "id": "JjmMejAly8F-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of columns with missing values that have to fill with mean\n",
        "columns_to_fill = [ 'glucose']\n",
        "\n",
        "# Iterate through the columns and fill NaN values with the mean of each column\n",
        "for col in columns_to_fill:\n",
        "    mean_value = cv_df[col].mean()\n",
        "    cv_df[col].fillna(mean_value, inplace=True)\n"
      ],
      "metadata": {
        "id": "O--zDo3iy_IQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Deleting the rest of the null values\n",
        "cv_df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "9vrTChybzSa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking if the null values are dropped properly\n",
        "cv_df.isnull().sum()"
      ],
      "metadata": {
        "id": "Ug20ZGIDzW9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Handling duplicate values:**"
      ],
      "metadata": {
        "id": "UB8GzuLuzeTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Checking for duplicate values\n",
        "cv_df.duplicated().sum()"
      ],
      "metadata": {
        "id": "YFm1Ag4TzjLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset doesnt contain any duplicate values.**"
      ],
      "metadata": {
        "id": "QpMS-Zlgzor6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Outliers:**"
      ],
      "metadata": {
        "id": "iiw61BzVzueA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature Engineering:**"
      ],
      "metadata": {
        "id": "JKXf4hKdzytP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Feature Encoding :**\n",
        "Machine Learning model work with numerical values therefore categorical columns have to converted/encoded into numerical variables.This process is known as Feature Encoding\n",
        "\n",
        "Here we have two columns that require encoding and they are \"sex\" and \"is_smoking\"."
      ],
      "metadata": {
        "id": "DE4Dyzwcz7_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Encoding the categorical columns\n",
        "cv_df['sex'] = cv_df['sex'].apply(lambda x: 1 if x == 'M' else 0)\n",
        "cv_df['is_smoking'] = cv_df['is_smoking'].apply(lambda x: 1 if x == 'YES' else 0)"
      ],
      "metadata": {
        "id": "HRjHoErT0Eg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Grouping columns for better Understanding :**"
      ],
      "metadata": {
        "id": "UTcaMe-v0IxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def smoke_pattern (cigperday:float):\n",
        "  \"\"\"A function that returns the Smoking level\n",
        "     by taking cigarettes per day as an input.\"\"\"\n",
        "\n",
        "  if cigperday==0:                    #Non smoker\n",
        "    return 1\n",
        "  elif cigperday>0 and cigperday<=10:       #Smoker with more than 0 and less than 10 cigs per day\n",
        "    return 2\n",
        "  elif cigperday>10 and cigperday<=20:      #Smoker with more than 10 and less than 20 cigs per day\n",
        "    return 3\n",
        "  elif cigperday>20 and cigperday<=30:      #Smoker with more than 20 and less than 30 cigs per day\n",
        "    return 4\n",
        "  elif cigperday>30 and cigperday<=40:      #Smoker with more than 30 and less than 40 cigs per day\n",
        "    return 5\n",
        "  else:                         #Smoker with more than 40 cigs per day\n",
        "    return 6"
      ],
      "metadata": {
        "id": "ocUOs3L70O4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the Smokepattern column\n",
        "cv_df['smoke_pattern'] = cv_df['cigsPerDay'].apply(lambda x: smoke_pattern(x))"
      ],
      "metadata": {
        "id": "FXUX-FKv0TgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Removing columns upon whom grouping has been done\n",
        "cv_df.drop(columns={'is_smoking','cigsPerDay'},axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "nS0pa6Ny1zu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_df.head()"
      ],
      "metadata": {
        "id": "9aGtH7CZ14ND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Smoke pattern column created with correct values."
      ],
      "metadata": {
        "id": "I_KV4erl19e2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **BPLevel:**"
      ],
      "metadata": {
        "id": "kHuGl1qW2CDO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, I will combine the \"sysBP\" and \"diaBP\" columns to create a new column called the \"BPLevel\"."
      ],
      "metadata": {
        "id": "O0488ZMs2MzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a function to assign blood pressure levels\n",
        "def bp_level(row):\n",
        "    if row['sysBP'] < 120 or row['diaBP'] < 80:\n",
        "        return 1  # Normal level\n",
        "    elif (120 <= row['sysBP'] < 130) or row['diaBP'] < 80:\n",
        "        return 2  # Elevated level\n",
        "    elif (130 <= row['sysBP'] < 140) or (80 <= row['diaBP'] < 90):\n",
        "        return 3  # High BP stage 1\n",
        "    elif (140 <= row['sysBP'] < 180) or (90 <= row['diaBP'] < 120):\n",
        "        return 4  # High BP stage 2\n",
        "    else:\n",
        "        return 5  # Hypertensive crisis\n",
        "\n",
        "# Create the 'BPLevel' column using the function\n",
        "cv_df['BPLevel'] = cv_df.apply(bp_level, axis=1)\n",
        "\n",
        "# Remove the 'sysBP' and 'diaBP' columns\n",
        "cv_df.drop(columns=['sysBP', 'diaBP'], inplace=True)\n",
        "\n",
        "# Checking if the 'BPLevel' column is created properly\n",
        "cv_df.head()"
      ],
      "metadata": {
        "id": "BIl-VzeF2Rut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DiabetesLevel:**"
      ],
      "metadata": {
        "id": "HZD5zodl2W_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to assign diabetes levels\n",
        "def diabetes_level(glucose):\n",
        "    if glucose < 53:\n",
        "        return 1  # Severe Hypoglycemia\n",
        "    elif 53 <= glucose < 70:\n",
        "        return 2  # Hypoglycemia\n",
        "    elif 70 <= glucose < 125:\n",
        "        return 3  # Normal\n",
        "    elif 125 <= glucose < 200:\n",
        "        return 4  # Pre Diabetic\n",
        "    else:\n",
        "        return 5  # Severe Diabetes\n",
        "\n",
        "# Create the 'DiabetesLevel' column using the function\n",
        "cv_df['DiabetesLevel'] = cv_df['glucose'].apply(diabetes_level)\n",
        "\n",
        "# Remove the 'diabetes' and 'glucose' columns\n",
        "cv_df.drop(columns=['diabetes', 'glucose'], inplace=True)\n",
        "\n",
        "# Checking if the 'DiabetesLevel' column is created properly\n",
        "cv_df.head()"
      ],
      "metadata": {
        "id": "Q0aLwr052cBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Checking correlation for feature removal:**"
      ],
      "metadata": {
        "id": "tlHhWZYp2hSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting correlation matrix using sns heatmap\n",
        "corr_matrix= cv_df.corr()\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(corr_matrix,annot=True,cmap='coolwarm')\n",
        "plt.title(\"Correlation between the variables of the dataset\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m_lKqoO_2mJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**There is no high correlation between majority variables but there for majority of the variables but there is a high correlation between \"prevalentHyp\" and \"BPLevel\". Here i will remove \"prevalentHyp\" because this is somehow direct related with \"BPLevel\" in mediacal terms.**"
      ],
      "metadata": {
        "id": "nOudLbIQbKG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove columns with high correlation\n",
        "cv_df.drop('prevalentHyp', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "1ly5BUBlbT2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Checking the distribution of the data:**"
      ],
      "metadata": {
        "id": "z3L5MMMRbX_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will reduce variables that do not contribute much in predicting the target variables."
      ],
      "metadata": {
        "id": "x0pC0HEYbdIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a list of all the independent variables\n",
        "independent_cols=list(set(cv_df.columns)-{'TenYearCHD'})"
      ],
      "metadata": {
        "id": "59nPhbWJbjB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n=1\n",
        "plt.figure(figsize=(14,30))\n",
        "for i in independent_cols:\n",
        "  plt.subplot(12,4,n)\n",
        "  n= n+1\n",
        "  sns.distplot(cv_df[i],color='teal')\n",
        "  plt.title(i)\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "Mise8yacbn4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "As we can see from the distribution, there is a high class imbalance for the columns BPMeds and prevalentStroke, so they won't be able to impact the prediction of the target variable much and therefore we'll delete them.\n",
        "\n",
        "From the EDA process we also saw that Education is not a great contributing factor, therefore I'll remove the education column also.\n",
        "\n",
        "Also id column is not useful so i am removing it ."
      ],
      "metadata": {
        "id": "TynBPQYSbzTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing useless columns\n",
        "cv_df.drop(columns={'BPMeds','prevalentStroke','education','id'},axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "CtxdKgE7bnzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dealing with class imbalance:**"
      ],
      "metadata": {
        "id": "Ibup5AAdb_A7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A dataset is imbalanced if the classification categories are not approximately equally represented. This affects the quality of our machine larning model and also causes a mistake of classifying the minority class as the majority class. Therefore we will try to deal with this class imbalance if it exists in our dataset."
      ],
      "metadata": {
        "id": "Z-QDsm_ncD48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking for class imbalance for the target variable\n",
        "cv_df['TenYearCHD'].value_counts()"
      ],
      "metadata": {
        "id": "Tt8lKtstcHAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, there is a high class imbalance here.\n",
        "\n",
        "Few echniques to solve class imbalance:\n",
        "\n",
        "1.Resampling (undersampling or oversampling)\n",
        "\n",
        "2.SMOTE\n",
        "\n",
        "3.Using BalancedBaggingClassifier.\n",
        "\n",
        "  and more....\n",
        "  \n",
        "In this project, to deal with class imbalance I will be using the SMOTE technique(synthetic minority oversampling technique)"
      ],
      "metadata": {
        "id": "Rp9Bmk-7cM9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SMOTE**"
      ],
      "metadata": {
        "id": "GMkvkMVqchED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Synthetic Minority Oversampling Technique (SMOTE) is a statistical technique for increasing the number of cases in your dataset in a balanced way. The component works by generating new instances from existing minority cases that you supply as input."
      ],
      "metadata": {
        "id": "tGf52jRsclyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the dataset for the independent and dependent variables.\n",
        "X = cv_df.drop('TenYearCHD', axis=1)\n",
        "Y = cv_df['TenYearCHD'].reset_index(drop=True)\n",
        "\n",
        "# Applying the SMOTE technique to solve class imbalance\n",
        "smote = SMOTE(sampling_strategy='minority')\n",
        "X_resampled, Y_resampled = smote.fit_resample(X, Y)\n",
        "\n",
        "# Displaying the first few rows of the resampled independent variables (X_resampled)\n",
        "X_resampled.head()"
      ],
      "metadata": {
        "id": "vf2DahxYcptJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_resampled.value_counts()"
      ],
      "metadata": {
        "id": "q6XctnbJctsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Class Imbalance is now removed.**"
      ],
      "metadata": {
        "id": "1n5hN4VlcxV0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Splitting and scalling the Data:**"
      ],
      "metadata": {
        "id": "DMGV5OSoc1bx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting the data\n",
        "X_train,X_test,Y_train,Y_test = train_test_split(X_resampled,Y_resampled,test_size=0.25,random_state=12)"
      ],
      "metadata": {
        "id": "wHqQbdFRcu8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Feature Scaling:**"
      ],
      "metadata": {
        "id": "pokT_mYuc91i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Scaling is a technique to standardize the independent features present in the data in a fixed range. It is performed during the data pre-processing to handle highly varying magnitudes or values or units. If feature scaling is not done, then a machine learning algorithm tends to weigh greater values, higher and consider smaller values as the lower values, regardless of the unit of the values."
      ],
      "metadata": {
        "id": "20PP6mE1dDT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**StandardScaler is used to resize the distribution of values ​​so that the mean of the observed values ​​is 0 and the standard deviation is 1.**"
      ],
      "metadata": {
        "id": "x_kF0krndGS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scale_numeric_columns(data):\n",
        "    scaler = StandardScaler()\n",
        "    numeric_data = data.select_dtypes(include=['number'])\n",
        "    scaled_data = scaler.fit_transform(numeric_data)\n",
        "    scaled_df = pd.DataFrame(scaled_data, columns=numeric_data.columns)\n",
        "    return scaled_df"
      ],
      "metadata": {
        "id": "r-IrwRh9cu5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Scaling the independent dataset\n",
        "X_train = scale_numeric_columns(X_train)\n",
        "X_test = scale_numeric_columns(X_test)"
      ],
      "metadata": {
        "id": "V6DliU2UdPKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Implementation:**"
      ],
      "metadata": {
        "id": "jFIalWUOdUVO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine learning models can be described as programs that are trained to find patterns or trends within data and predict the result for new data.\n",
        "\n",
        "In this project we are dealing with a classification problem, therefore we will be using classification models.\n",
        "\n",
        "In this project we will be including the following models:\n",
        "\n",
        "1.Logistic regression.\n",
        "\n",
        "2.Decision tree classifier.\n",
        "\n",
        "3.Random forest classifier.\n",
        "\n",
        "4.Gradient Boosting classifier."
      ],
      "metadata": {
        "id": "4l4u1KsMdZqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:**\n",
        "\n",
        "All these models have similar training and predicting processes, so writing code for each one of them seperately makes it quite boring and lengthy. To solve this problem we can use the concept of ML pipelines. To implement this I will be using functions to execute the ML model trainings and also to evaluate the ML models."
      ],
      "metadata": {
        "id": "VZeQmv1Cdd6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Performance Metrics:**"
      ],
      "metadata": {
        "id": "ZLOcEIF4dxTu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different performance metrics are used to evaluate machine learning model. Based on our task we can choose our performance metrics. Since our task is of classification and that too binary class classification, whether client will or will not subscribe for deposits.\n",
        "\n",
        "Here we will be using AUC ROC"
      ],
      "metadata": {
        "id": "G_RVxqC7d3jS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Performance Metrics\n",
        "def model_evaluator(actual, preds, ml_model, mode):\n",
        "    cm = confusion_matrix(actual, preds)\n",
        "    print(\"Confusion Matrix:\\n\", cm, '\\n')\n",
        "    sns.heatmap(cm, annot=True, cmap='coolwarm', fmt='d')\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('Actual Labels')\n",
        "    plt.title(f'Confusion Matrix for {ml_model} on the {mode} set')\n",
        "    plt.show()\n",
        "\n",
        "    roc_auc = roc_auc_score(actual, preds)\n",
        "    print('\\nROC AUC Score:', roc_auc)\n",
        "\n",
        "    print('\\nClassification Report:\\n')\n",
        "    target_names = ['Class 0', 'Class 1']\n",
        "    print(classification_report(actual, preds, target_names=target_names))\n",
        "\n",
        "# Model Pipeline\n",
        "def model_pipeline(X_train, X_test, Y_train, Y_test, ml_model, param_grid=None, kind='evaluate'):\n",
        "    model = None\n",
        "\n",
        "    if ml_model == 'Logistic Regression':\n",
        "        model = LogisticRegression(random_state=12)\n",
        "    elif ml_model == 'Decision Tree Classifier':\n",
        "        model = DecisionTreeClassifier()\n",
        "    elif ml_model == 'Random Forest Classifier':\n",
        "        model = RandomForestClassifier()\n",
        "    elif ml_model == 'Gradient Boosting Classifier':\n",
        "        model = GradientBoostingClassifier()\n",
        "    else:\n",
        "        print(\"Enter correct model name: Logistic Regression, Decision Tree Classifier, Random Forest Classifier, or Gradient Boosting Classifier.\")\n",
        "        return\n",
        "\n",
        "    if param_grid:\n",
        "        gs_model = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='roc_auc', verbose=True)\n",
        "        gs_model.fit(X_train, Y_train)\n",
        "        print(\"Best parameters for\", ml_model, \":\", gs_model.best_params_, '\\n')\n",
        "        model = gs_model.best_estimator_\n",
        "\n",
        "    model.fit(X_train, Y_train)\n",
        "    train_predictions = model.predict(X_train)\n",
        "    test_predictions = model.predict(X_test)\n",
        "\n",
        "    if kind == 'evaluate':\n",
        "        print(\"1. Train set evaluation:\")\n",
        "        model_evaluator(Y_train, train_predictions, ml_model, 'Train')\n",
        "        print(\"\\n2. Test set evaluation:\")\n",
        "        model_evaluator(Y_test, test_predictions, ml_model, 'Test')\n",
        "    elif kind == 'model_explainability':\n",
        "        return model"
      ],
      "metadata": {
        "id": "GIA8koP3d_Lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameter grids\n",
        "param_grid_dt = {\n",
        "    'max_depth': [4, 6, 8, 10],\n",
        "    'min_samples_split': [5, 10, 20, 30, 40, 50],\n",
        "    'min_samples_leaf': [5, 10, 15, 20]\n",
        "}\n",
        "\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [50, 65, 80, 95, 120],\n",
        "    'max_depth': [3, 5, 7, 9, 10]\n",
        "}\n",
        "\n",
        "param_grid_gb = {\n",
        "    'n_estimators': [80, 100],\n",
        "    'max_depth': [5, 7, 8],\n",
        "    'learning_rate': [0.001, 0.01, 0.05]\n",
        "}"
      ],
      "metadata": {
        "id": "F_0gzQHMd_Gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Logistic Regression:**"
      ],
      "metadata": {
        "id": "I2D89KO_eM-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Evluate Decision Tree model\n",
        "model_pipeline(X_train, X_test, Y_train, Y_test, ml_model='Logistic Regression')"
      ],
      "metadata": {
        "id": "r33ioXAZeLDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Decision Tree Classifier:**"
      ],
      "metadata": {
        "id": "nTZSNVuGehOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_pipeline(X_train, X_test, Y_train, Y_test, ml_model='Decision Tree Classifier', param_grid=param_grid_dt, kind='evaluate')"
      ],
      "metadata": {
        "id": "nYXMKR6QerDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Ranndom Forest Classifier**"
      ],
      "metadata": {
        "id": "cE27A3_6fW2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_pipeline(X_train, X_test, Y_train, Y_test, ml_model='Random Forest Classifier', param_grid=param_grid_dt, kind='evaluate')"
      ],
      "metadata": {
        "id": "g0mB0EqIeyD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gradient Boosting Classifier**"
      ],
      "metadata": {
        "id": "JJBOXWiXfcQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_pipeline(X_train, X_test, Y_train, Y_test, ml_model='Gradient Boosting Classifier', param_grid=param_grid_gb, kind='evaluate')"
      ],
      "metadata": {
        "id": "Qqay6q_Bflq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Explainability:**"
      ],
      "metadata": {
        "id": "oDay5atxfql-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing the shap library\n",
        "!pip install shap"
      ],
      "metadata": {
        "id": "ZGB-26uifyjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the SHAP library\n",
        "import shap"
      ],
      "metadata": {
        "id": "y9yWvIbFf2NE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating an object for the logistic regression model\n",
        "lr_classifier = model_pipeline(X_train,X_test,Y_train,Y_test, ml_model='Logistic Regression',kind='model_explainability')"
      ],
      "metadata": {
        "id": "AyhhL6xNf5rJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explainer for SHAP values\n",
        "explainer = shap.Explainer(lr_classifier, X_train)\n",
        "shap_values = explainer(X_train)\n",
        "\n",
        "# Plot SHAP summary\n",
        "shap.summary_plot(shap_values, X_train, feature_names=X_train.columns)"
      ],
      "metadata": {
        "id": "-zjDfEe2f--s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature importance function for tree-based models\n",
        "def feature_importance(model, feature_names):\n",
        "    importances = model.feature_importances_\n",
        "    indices = np.argsort(importances)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.title('Feature Importance')\n",
        "    plt.barh(range(len(indices)), importances[indices], color='lightgreen', align='center')\n",
        "    plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
        "    plt.xlabel('Relative Importance')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "RMeqWx2Cf-6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define parameter grids\n",
        "param_grid_dt = {\n",
        "    'max_depth': [4, 6, 8, 10],\n",
        "    'min_samples_split': [5, 10, 20, 30, 40, 50],\n",
        "    'min_samples_leaf': [5, 10, 15, 20]\n",
        "}\n",
        "\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [50, 65, 80, 95, 120],\n",
        "    'max_depth': [3, 5, 7, 9, 10]\n",
        "}\n",
        "\n",
        "\n",
        "param_grid_gb = {\n",
        "    'n_estimators': [80, 100],\n",
        "    'max_depth': [5, 7, 8],\n",
        "    'learning_rate': [0.001, 0.01, 0.05]}"
      ],
      "metadata": {
        "id": "Zg8X4t1gf-03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train and explain Decision Tree model using feature importance\n",
        "dt_classifier = model_pipeline(X_train, X_test, Y_train, Y_test, ml_model='Decision Tree Classifier', param_grid=param_grid_dt, kind='model_explainability')\n",
        "feature_importance(dt_classifier, X_train.columns)"
      ],
      "metadata": {
        "id": "bZFW3i6yhH_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and explain Random Forest model using feature importance\n",
        "rf_classifier = model_pipeline(X_train, X_test, Y_train, Y_test, ml_model='Random Forest Classifier', param_grid=param_grid_rf, kind='model_explainability')\n",
        "feature_importance(rf_classifier, X_train.columns)"
      ],
      "metadata": {
        "id": "Q25V2gJQhH8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and explain Random Forest model using feature importance\n",
        "gb_classifier = model_pipeline(X_train, X_test, Y_train, Y_test, ml_model='Gradient Boosting Classifier', param_grid=param_grid_gb, kind='model_explainability')\n",
        "feature_importance(gb_classifier, X_train.columns)"
      ],
      "metadata": {
        "id": "KKjtUfgvhPOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusion:**"
      ],
      "metadata": {
        "id": "fy8as42uiIPA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EDA Insights:**\n",
        "\n",
        "1. People who have age between 47 to 65 have high chances of Positive CHD.\n",
        "\n",
        "2. Being a Male has increase chances of Positive CHD.\n",
        "\n",
        "3. People who are smoking has high chances of Positive CHD.\n",
        "\n",
        "4. Persons taking BP Medicines has more chances of Positive CHD.\n",
        "\n",
        "5. Prevalent Stroke and Prevalent Hypertension are also major factors which\n",
        "   increases chances of CHD.\n",
        "\n",
        "6. Person with Diabetes are more prone to Positive Coronary Heart Diseases."
      ],
      "metadata": {
        "id": "U-KWSqqgiQBi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results from ML Model:**\n",
        "\n",
        "1.Logistic regression gives a ROCAUC score of 0.6277 on the testing set. This is worst performing model.\n",
        "\n",
        "2.Decision tree model gives a ROCAUC score of 0.7069 on the testing set.\n",
        "\n",
        "3.Random Forest Classifier model gives a ROCAUC score of 0.7533 on the testing set. This is the best performing model.\n",
        "\n",
        "4.Gradient Boosting Classifier model gives a ROCAUC score of 0.7520 on the testing set.\n",
        "\n",
        "5.Model explainability has been achieved by SHAP library's summary plot and an attribute called feature_importance_ of the tree based algorithms.\n",
        "\n",
        "6.Total cholestrol and age are the two most important factors to predict the CHD risk factor."
      ],
      "metadata": {
        "id": "sexY0DNEinaG"
      }
    }
  ]
}